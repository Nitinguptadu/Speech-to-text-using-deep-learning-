{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defines a functions for training a NN.\n",
    "\"\"\"\n",
    "\n",
    "from data_generator import AudioGenerator\n",
    "import _pickle as pickle\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Lambda)\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "import os\n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "def add_ctc_loss(input_to_softmax):\n",
    "    the_labels = Input(name='the_labels', shape=(None,), dtype='float32')\n",
    "    input_lengths = Input(name='input_length', shape=(1,), dtype='int64')\n",
    "    label_lengths = Input(name='label_length', shape=(1,), dtype='int64')\n",
    "    output_lengths = Lambda(input_to_softmax.output_length)(input_lengths)\n",
    "    # CTC loss is implemented in a lambda layer\n",
    "    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')(\n",
    "        [input_to_softmax.output, the_labels, output_lengths, label_lengths])\n",
    "    model = Model(\n",
    "        inputs=[input_to_softmax.input, the_labels, input_lengths, label_lengths], \n",
    "        outputs=loss_out)\n",
    "    return model\n",
    "\n",
    "def train_model(input_to_softmax, \n",
    "                pickle_path,\n",
    "                save_model_path,\n",
    "                train_json='train_corpus.json',\n",
    "                valid_json='valid_corpus.json',\n",
    "                minibatch_size=20,\n",
    "                spectrogram=True,\n",
    "                mfcc_dim=13,\n",
    "                optimizer=SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5),\n",
    "                epochs=20,\n",
    "                verbose=1,\n",
    "                sort_by_duration=False,\n",
    "                max_duration=10.0):\n",
    "    \n",
    "    # create a class instance for obtaining batches of data\n",
    "    audio_gen = AudioGenerator(minibatch_size=minibatch_size, \n",
    "        spectrogram=spectrogram, mfcc_dim=mfcc_dim, max_duration=max_duration,\n",
    "        sort_by_duration=sort_by_duration)\n",
    "    # add the training data to the generator\n",
    "    audio_gen.load_train_data(train_json)\n",
    "    audio_gen.load_validation_data(valid_json)\n",
    "    # calculate steps_per_epoch\n",
    "    num_train_examples=len(audio_gen.train_audio_paths)\n",
    "    steps_per_epoch = num_train_examples//minibatch_size\n",
    "    # calculate validation_steps\n",
    "    num_valid_samples = len(audio_gen.valid_audio_paths) \n",
    "    validation_steps = num_valid_samples//minibatch_size\n",
    "    \n",
    "    # add CTC loss to the NN specified in input_to_softmax\n",
    "    model = add_ctc_loss(input_to_softmax)\n",
    "\n",
    "    # CTC loss is implemented elsewhere, so use a dummy lambda function for the loss\n",
    "    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=optimizer)\n",
    "\n",
    "    # make results/ directory, if necessary\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "\n",
    "    # add checkpointer\n",
    "    checkpointer = ModelCheckpoint(filepath='results/'+save_model_path, verbose=0)\n",
    "\n",
    "    # train the model\n",
    "    hist = model.fit_generator(generator=audio_gen.next_train(), steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs, validation_data=audio_gen.next_valid(), validation_steps=validation_steps,\n",
    "        callbacks=[checkpointer], verbose=verbose)\n",
    "\n",
    "    # save model loss\n",
    "    with open('results/'+pickle_path, 'wb') as f:\n",
    "        pickle.dump(hist.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "\n",
    "\n",
    "\n",
    "def rnn_model(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units, activation=activation,\n",
    "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 200)         217200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 223,829\n",
      "Trainable params: 223,429\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_1 = rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                    units=200,\n",
    "                    activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "106/106 [==============================] - 129s 1s/step - loss: 290.4148 - val_loss: 313.9343\n",
      "Epoch 2/20\n",
      "106/106 [==============================] - 135s 1s/step - loss: 202.1477 - val_loss: 191.8466\n",
      "Epoch 3/20\n",
      "106/106 [==============================] - 134s 1s/step - loss: 180.8319 - val_loss: 186.3640\n",
      "Epoch 4/20\n",
      "106/106 [==============================] - 133s 1s/step - loss: 163.8422 - val_loss: 162.8869\n",
      "Epoch 5/20\n",
      "106/106 [==============================] - 135s 1s/step - loss: 154.5856 - val_loss: 179.2887\n",
      "Epoch 6/20\n",
      "106/106 [==============================] - 134s 1s/step - loss: 148.9711 - val_loss: 164.4835\n",
      "Epoch 7/20\n",
      "106/106 [==============================] - 131s 1s/step - loss: 143.4073 - val_loss: 153.5095\n",
      "Epoch 8/20\n",
      "106/106 [==============================] - 131s 1s/step - loss: 139.5173 - val_loss: 162.3356\n",
      "Epoch 9/20\n",
      "106/106 [==============================] - 132s 1s/step - loss: 136.8279 - val_loss: 143.0319\n",
      "Epoch 10/20\n",
      "106/106 [==============================] - 138s 1s/step - loss: 133.2455 - val_loss: 150.6595\n",
      "Epoch 11/20\n",
      "106/106 [==============================] - 133s 1s/step - loss: 130.5170 - val_loss: 186.1750\n",
      "Epoch 12/20\n",
      "106/106 [==============================] - 131s 1s/step - loss: 129.3398 - val_loss: 126.1608\n",
      "Epoch 13/20\n",
      "106/106 [==============================] - 137s 1s/step - loss: 126.9690 - val_loss: 125.0379\n",
      "Epoch 14/20\n",
      "106/106 [==============================] - 137s 1s/step - loss: 125.4136 - val_loss: 143.2319\n",
      "Epoch 15/20\n",
      "106/106 [==============================] - 132s 1s/step - loss: 124.1208 - val_loss: 142.9257\n",
      "Epoch 16/20\n",
      "106/106 [==============================] - 132s 1s/step - loss: 121.0770 - val_loss: 126.0803\n",
      "Epoch 17/20\n",
      "106/106 [==============================] - 133s 1s/step - loss: 120.8862 - val_loss: 166.2095\n",
      "Epoch 18/20\n",
      "106/106 [==============================] - 131s 1s/step - loss: 121.0891 - val_loss: 165.6129\n",
      "Epoch 19/20\n",
      "106/106 [==============================] - 131s 1s/step - loss: 122.4181 - val_loss: 98.4736\n",
      "Epoch 20/20\n",
      "106/106 [==============================] - 143s 1s/step - loss: 119.9497 - val_loss: 151.6153\n"
     ]
    }
   ],
   "source": [
    "train_model(input_to_softmax=model_1, \n",
    "            pickle_path='model_1.pickle', \n",
    "            save_model_path='model_1.h5',\n",
    "            spectrogram=True) # change to False if you would like to use MFCC features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from data_generator import AudioGenerator\n",
    "from keras import backend as K\n",
    "from utils import int_sequence_to_text\n",
    "from IPython.display import Audio\n",
    "\n",
    "def get_predictions(index, partition, input_to_softmax, model_path):\n",
    "    \"\"\" Print a model's decoded predictions\n",
    "    Params:\n",
    "        index (int): The example you would like to visualize\n",
    "        partition (str): One of 'train' or 'validation'\n",
    "        input_to_softmax (Model): The acoustic model\n",
    "        model_path (str): Path to saved acoustic model's weights\n",
    "    \"\"\"\n",
    "    # load the train and test data\n",
    "    data_gen = AudioGenerator()\n",
    "    data_gen.load_train_data()\n",
    "    data_gen.load_validation_data()\n",
    "    \n",
    "    # obtain the true transcription and the audio features \n",
    "    if partition == 'validation':\n",
    "        transcr = data_gen.valid_texts[index]\n",
    "        audio_path = data_gen.valid_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    elif partition == 'train':\n",
    "        transcr = data_gen.train_texts[index]\n",
    "        audio_path = data_gen.train_audio_paths[index]\n",
    "        data_point = data_gen.normalize(data_gen.featurize(audio_path))\n",
    "    else:\n",
    "        raise Exception('Invalid partition!  Must be \"train\" or \"validation\"')\n",
    "        \n",
    "    # obtain and decode the acoustic model's predictions\n",
    "    input_to_softmax.load_weights(model_path)\n",
    "    prediction = input_to_softmax.predict(np.expand_dims(data_point, axis=0))\n",
    "    output_length = [input_to_softmax.output_length(data_point.shape[0])] \n",
    "    pred_ints = (K.eval(K.ctc_decode(\n",
    "                prediction, output_length)[0][0])+1).flatten().tolist()\n",
    "    \n",
    "    # play the audio file, and display the true and predicted transcriptions\n",
    "    print('-'*80)\n",
    "    Audio(audio_path)\n",
    "    print('True transcription:\\n' + '\\n' + transcr)\n",
    "    print('-'*80)\n",
    "    print('Predicted transcription:\\n' + '\\n' + ''.join(int_sequence_to_text(pred_ints)))\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (BatchNormalization, Conv1D, Dense, Input, \n",
    "    TimeDistributed, Activation, Bidirectional, SimpleRNN, GRU, LSTM)\n",
    "\n",
    "\n",
    "\n",
    "def rnn_model(input_dim, units, activation, output_dim=29):\n",
    "    \"\"\" Build a recurrent network for speech \n",
    "    \"\"\"\n",
    "    # Main acoustic input\n",
    "    input_data = Input(name='the_input', shape=(None, input_dim))\n",
    "    # Add recurrent layer\n",
    "    simp_rnn = GRU(units, activation=activation,\n",
    "        return_sequences=True, implementation=2, name='rnn')(input_data)\n",
    "    # TODO: Add batch normalization \n",
    "    bn_rnn = BatchNormalization()(simp_rnn)\n",
    "    # TODO: Add a TimeDistributed(Dense(output_dim)) layer\n",
    "    time_dense = TimeDistributed(Dense(output_dim))(bn_rnn)\n",
    "    # Add softmax activation layer\n",
    "    y_pred = Activation('softmax', name='softmax')(time_dense)\n",
    "    # Specify the model\n",
    "    model = Model(inputs=input_data, outputs=y_pred)\n",
    "    model.output_length = lambda x: x\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "the_input (InputLayer)       (None, None, 161)         0         \n",
      "_________________________________________________________________\n",
      "rnn (GRU)                    (None, None, 200)         217200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 200)         800       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, None, 29)          5829      \n",
      "_________________________________________________________________\n",
      "softmax (Activation)         (None, None, 29)          0         \n",
      "=================================================================\n",
      "Total params: 223,829\n",
      "Trainable params: 223,429\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model_1 = rnn_model(input_dim=161, # change to 13 if you would like to use MFCC features\n",
    "                    units=200,\n",
    "                    activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "True transcription:\n",
      "\n",
      "he had neither a national army nor an organized church\n",
      "--------------------------------------------------------------------------------\n",
      "Predicted transcription:\n",
      "\n",
      "he a ne theri aso e o oren as trer t\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_predictions(index=0, \n",
    "                partition='train',\n",
    "                input_to_softmax=model_1, \n",
    "                model_path='results/model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
